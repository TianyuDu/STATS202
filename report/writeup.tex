\documentclass[11pt]{article}
\usepackage{spikey}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{mathrsfs}
\usepackage{centernot}
\usepackage[shortlabels]{enumitem}
\usepackage{verbatim}

\usepackage[margin=1truein]{geometry}
\usepackage{setspace}
\linespread{1.15}

\title{Analysis on PANSS Dataset\\ \small STATS 202: Data Mining and Analysis, Final Project}
\author{Tianyu Du}
\date{\today}

\begin{document}
	\maketitle
	\tableofcontents
%	\newpage

	\section{Introduction}
%	\subsection{PANSS Dataset}
	\paragraph{}The Positive and Negative Syndrome Scale (PANSS) score is widely used as a measure for schizophrenia and other disorders in clinical trials. PANSS scores are collected by trained raters and reported by patients or their relatives. One assessment of PANSS scores consists 30 sub-scores from 3 sub-categories: 7 positive scores, 7 negative scores, and 16 general scores. Every score ranges from 1 to 7 denoting increasing levels of psychopathology. The aggregation of all 30 scores provides a detailed assessment of patient's current psychological status.
	
	\paragraph{}The entire dataset consists of five different studies ranging from study A to study E. In this practice, the first four datasets are used as a training to fit, select, and evaluate models. Then, these selected models are used to recover missing data in study E.
	
	\section{Treatment Effects}
	\paragraph{}This section is devoted to analyze whether the treatment assigned leads to significant improvements on patients' psychological status. Because the 18-th week assessments are missing in the dataset of study E, in this section, only data from study A to D are considered. There are 20947 observations (assessments) from above-mentioned dataset, in which 10524 observations came from patients assigned to the control group, and the remaining 10423 were from participants belonged to the treatment group. The evenly-split dataset allows us to deploy various models to analyze whether there exists significant treatment effects.
	
	\paragraph{}Multiple evidences were found to support that there were indeed no treatment effect in this study. Firstly, the effects on four aggregate scores are analyzed, namely sum of all PANSS scores, and sums of scores from positive (\texttt{P\_Total}), negative (\texttt{N\_Total}), and general (\texttt{G\_Total}) sub-categories respectively.
	
	\paragraph{}One challenge associated with treatment effect analysis is the initial status of patients in different groups. In some cases, the \emph{prior} psychological status for a randomly selected patient from the treatment group is expected to be different from that of a random patient in the control group. In these cases, even if significant differences in PANSS scores \emph{posterior} to the treatment were supported, one will not be able to distinguish whether the "effect" came from the discrepancy in the prior distributions or the treatment, and the treatment effect is not well-identifies.
	Figure 1 below presents the distributions of the four aggregate scores at day 0 visit. Both groups shared similar histograms in terms of all four metrics. Moreover, the estimated kernel densities collided, which provided further evidence that there were no significant prior discrepancies between patients from these two groups. Therefore, one can conclude that most posterior differences in PANSS metrics were resulted from the treatment assigned.

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{figures/dist_initial_scores.png}
		\caption{Distributions of Aggregate Scores at Day 0}
	\end{figure}

	\paragraph{}The evolving path of these four above mentioned metrics can provide reasonable proxies to the treatment effect. As mentioned before, we believed the data failed to provide sufficient evidence to support the existence of treatment effect. Let $t \in \N$ denote the \texttt{VisitDay} variable in the dataset, $t$ ranges from 0 to 480 in the complete dataset. Figure 2 presents the distribution of $t$, the 95-percent quantile is located at $t_{95\%} = 297$. Therefore, the top 5\% of observations occupied more than on third of the total range of $t$, these observations are potentially troublesome as outliers. To deal with this issue, the top 5\% observations are excluded from following analysis.

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{figures/dist_visit_day_all.png}
		\caption{Distribution of \texttt{VisitDay}}
	\end{figure}
	
	\paragraph{}The general strategy to identify treatment effect here uses linear models. Let $Y$ denote the target metric, which takes value from the four aggregate PANSS metrics: $\mc{M} := \{ \Sigma_{PANSS}, \Sigma_P, \Sigma_N, \Sigma_G\}$. Let $X$ denote the set other characteristics of the patient. And a hybrid linear model is fit:
	\begin{align}
		Y &= f_0(X, t) + \id{\texttt{Treatment}} f_1(X, t)
	\end{align}
	where $f_0$ and $f_1$ are two additive linear models, so that $f_0$ captures the evolving path of $Y$ over time for the population of the control group, and the additive term $f_1$ measures the discrepancy of the treatment group.
	Should the treatment dummy variable is statistically significant, then one can conclude the existence of treatment effect.
	\subsection{Patterns of PANSS over Time}
	\paragraph{}Figure 3 below shows the scatter plot of $\Sigma_{PANSS}$ scores against time for both groups with corresponding locally weighted linear regression estimations (LOWESS). The LOWESS for the treatment and control groups almost collide perfectly, which means the trends for both group estimated from non-parametric model are essentially identical, this provides preliminary evidence supporting our claim that there was no significant treatment effect.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{figures/lwlm_te_PANSS_Total.png}
		\caption{Changes of \texttt{PANSS\_Total} Over Time and LOWESS}
	\end{figure}
	\subsection{Identifying Treatment Effect with Hypothesis Testing}
	\paragraph{} As the plot in figure 3 suggests, the relationship between PANSS scores and $t$ is more or less quadratic. Moreover, similar quadratic characteristics were also found on the other three aggregate metrics (see appendix). Therefore, $f_0$ and $f_1$ are identified as quadratic functions of $t$, specifically,
	\begin{align}
		Y &= f_0(X, t) + \id{\texttt{Treatment}} f_1(X, t) \\
		&= \left(\beta_{0, 0} + \beta_{0, 1} t + \beta_{0, 2} t^2 + \vec{\gamma}_0 \texttt{Country} + \vec{\delta}_0 \texttt{Study}\right) \\
		&+ \id{\texttt{Treatment}} \left(\beta_{1, 0} + \beta_{1, 1} t + \beta_{1, 2} t^2 + \vec{\gamma}_1 \texttt{Country} + \vec{\delta}_1 \texttt{Study}\right) + \varepsilon \\
		&= \theta_0 + \theta_1 t + \theta_2 t^2 + \theta_3 \id{\texttt{Treatment}} + \theta_4 \id{\texttt{Treatment}} t + \theta_5 \id{\texttt{Treatment}} t^2 + \\
		&+\vec{\gamma}_0 \texttt{Country} + \vec{\delta}_0 \texttt{Study} + Z + \varepsilon
	\end{align}
	where $\texttt{Study}$ and $\texttt{Country}$ denote the collection of dummy variables, and $Z$ denotes the set of interaction terms between variables in $X$ and $\id{\texttt{Treatment}}$. It could be controversial whether to include interacting terms between $\id{\texttt{Treatment}}$ and variables other than $t$. As a result, regression results under all combinations of optional covariates are reported in table 1\footnote{Significance codes:  $0 *** 0.001 ** 0.01 * 0.05 \dagger 0.1 \quad 1$}, in which \texttt{S}, \texttt{C}, and \texttt{T} stand for \texttt{Study}, \texttt{Country}, and \texttt{Treatment} respectively. The negative coefficients of $\texttt{T} \times t$ in different settings suggest the treatment actually helped reduce the severity of schizophrenia. However, due to the large standard error of estimator, almost all of these coefficients were found insignificant. Even though in some rare cases, the treatment indicator on the intercept term were different from zero significantly. Consequently, we conclude that the treatment did no affect how PANSS metrics changed over time, so there are no treatment effect.
	\begin{table}[H]
	\centering
	\begin{tabular}{l|c|c|c|c}
		Covariates from $Z$ & $\varnothing$ & \texttt{C}*\texttt{T} & \texttt{S}*\texttt{T} & \texttt{C}*\texttt{T} + \texttt{S}*\texttt{T}\\
		\hline
		\texttt{PANSS\_Total} $\sim$ \texttt{T}& $0.5167(0.1873)^{**}$& -2.113(1.577)& -0.4924(0.5505) & -3.717(1.798)$^*$ \\
		\texttt{PANSS\_Total} $\sim$ $\texttt{T} \times t$ & -35.95(26.40)& -31.33(26.91)& -36.58(27.18)& -2.811(2.736)\\
		\texttt{PANSS\_Total} $\sim$ $\texttt{T} \times t^2$ & 24.99 (26.36) & 23.72(26.36)& 32.84(26.40)& 2.819(2.641)\\
		\hline 
		\texttt{P\_Total} $\sim$ \texttt{T}& 0.06806(0.06473)& -1.296(0.5447)$^*$ & -0.6036(0.1902) & -2.294(0.6210)$^{***}$\\
		\texttt{P\_Total} $\sim$ $\texttt{T} \times t$ & -7.643(9.121) & -8.135(9.297)& -12.59(9.391)$^{**}$ & -10.58(9.453)\\
		\texttt{P\_Total} $\sim$ $\texttt{T} \times t^2$ & 14.65(9.107)& 1.449(9.108)& 17.65(9.122)$^\dagger$& 14.89(9.126)\\
		\hline
		\texttt{N\_Total} $\sim$ \texttt{T}& 0.1132(0.06793)$^\dagger$& -0.1981(0.5700) & -0.002971(0.1996)& -0.3724(0.6498)\\
		\texttt{N\_Total} $\sim$ $\texttt{T} \times t$ & -10.22(9.571)& -6.610(9.728) & -6.791(9.856) & -2.909(9.890)\\
		\texttt{N\_Total} $\sim$ $\texttt{T} \times t^2$ & 0.4369(9.557)& 1.472(9.530) & 3.389(9.573)& 4.399(9.548)\\
		\hline
		\texttt{G\_Total} $\sim$ \texttt{T}& 0.3353(0.09998)$^{***}$&  -0.6194(0.8418) &  0.1142(0.2939) & -1.051(0.9601)\\
		\texttt{G\_Total} $\sim$ $\texttt{T} \times t$ & -18.09(14.09) & -16.49(14.37)& -17.20(14.51)& -14.62(14.61)\\
		\texttt{G\_Total} $\sim$ $\texttt{T} \times t^2$ & 9.901 (14.07)& 7.751(14.08) & 11.80(14.10)& 8.900(14.12)\\
	\end{tabular}
	\caption{Regression Results with Different Combination of Variables (Standard Error in Parenthesis)}
	\end{table}

	
	\section{Patient Segmentation}
	\section{Patient 18-th Week PANSS Forecasting}

	\paragraph{} This section is devoted to forecasting the total PANSS scores in the last visit of each participating patient. Several variables including indicator for treatment and country were invariant throughout the experiment period. The binary \texttt{TxGroup} variable was simply reduced to one binary variable \texttt{Treatment} $:= \id{\texttt{TxGroup} = \tx{Treatment}}$.
	
	\paragraph{} There are 284 unique site IDs and 639 unique rater IDs. Because IDs are numerical but not ordinal values, adding these two features would require an addition of more than 900 one-hot variable, which is significantly larger than the number of raw PANSS scores. Including them could be helpful, but at a risk of potential overfitting and cruise of dimensionality. Therefore, these IDs were excluded.
	\paragraph{} Assessments in the dataset came from 27 different countries (figure below). Note that there are 18 assessments (belong to 3 patients) do not possess valid values for country. To reduce the dimension of feature space, and handle issue when there are incoming patients from countries not in the training set (there were patients from UK in the test set, but not in the training set), only information on the top five countries was preserved, and all other countries were reduced to one single "other" category. As a result, the \texttt{Country} feature in the raw dataset was transformed into six one-hot-encoded dummies: \texttt{Country\_USA}, \texttt{Country\_Ukraine}, \texttt{Country\_Japan}, \texttt{Country\_Russia}, \texttt{Country\_China}, and \texttt{Country\_Other}.

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{figures/dist_country.png}
		\caption{Distribution of \texttt{Country}}
	\end{figure}


	\section{Assessment Validity Classification}
	\subsection{Feature Space}
	\paragraph{} In this section, each assessment becomes one training instance. The feature space consists of 30 PANSS scores and several identifiers such as country and rater's ID. In this study, all 30 PANSS scores and \texttt{PANSS\_Total} were taken to be preliminary features. As for treatment and country identifiers, the same procedure mentioned in the previous section was followed.
	\paragraph{} In the training set, the standard deviations of 30 PANSS sub-scores range from 0.9374 to 1.562, and their averages range from 1.572 to 3.258. To eliminate this discrepancy, These 30 metrics were standardized so that all of them share mean of zero and standard deviation of one.
	\paragraph{} Moreover, the figure below plots out the fraction for an assessment taken on particular visiting day to be flagged or assigned to CS. The plot suggests a nontrivial relation between the empirical probability of anomaly assessment and the day of visit. Therefore, \texttt{VisitDay} is also included as a feature.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{figures/alert_ratio_days.png}
		\caption{Fraction of Assessments Flagged on Each Day}
	\end{figure}
	
	\subsection{General Strategy to Identify the Best Learner}
	\paragraph{} In the following parts of this section, various models are proposed for the classification task using features mentioned before.
	\paragraph{} In general, the hyper-parameters of each model must be tuned over a large hyper-parameter space $\mc{M}$. However, it is infeasible to search over the entire hyper-parameter space as there are uncountably infinite combination of hyper-parameters for each model. Instead, a grid search algorithm with cross-validation was deployed. Firstly, a finite subset of $\mc{M}$, $\mc{G}$, was constructed manually, in which each element $g \in \mc{G}$ characterizes a valid model. Then for each $g \in \mc{G}$, the grid search algorithm fit a model with hyper-parameter set $g$ five times on different training sets given by a 5-fold cross validation.
	\footnote{Due to the time constraint, 5-fold CV was used instead of the more conventional 10-fold CV. Typically, hundreds of possible configurations were searched over for each type of model, using $k$-fold CV requires $k \times |\mc{G}|$ model fitting. While $k=5$, the grid search for each type of model took around 4 hours on a 64-core server, it seemed to be infeasible to run grid search with $k=10$.}
	And the generalization error for such model is estimated using the collection of different CV test sets. Then, for each category of models (random forest, neural nets, etc), the hyper-parameter set  achieved the best CV performance measured by cross-entropy loss was saved to represent the best performance for this model category.
	
	 \paragraph{} After the best hyper-parameter set for each type of learner was identify, they were evaluated again using $k$-fold cross validation techniques to make comparisons across different types of models, where $k$ depends on the actual time taken to train particular model.
	
	\subsection{Baseline Model: Logistic Regression}
	\paragraph{} A logistic regression model is used as a performance baseline.
 	
 	\subsection{Ensemble Model: Random Forest}
 	\paragraph{} Random forests handles overfitting problems naturally as an ensemble method. Several hyper-parameters play crucial rules while fitting a random forest, including number of tree built, the criterion for choosing the best split, the number of features to consider while identifying the best split, as well as the maximum depth of each tree. Let $\mc{G}$ and $p$ denote the proposed hyper-parameter space and number of features respectively. The table below shows the scope of above mentioned hyper-parameters searched over during the grid search process.
 	\begin{table}[H]
 		\centering
 		\begin{tabular}{l|c|c|c}
 		H-Param & Range & Best & Total \\
 		\hline
 		Max Depth ($\delta$) & $\{\infty, 2, 4, \cdots, 1024\}$ & 64 & 7 \\
 		Num Trees ($\tau$) & $\{100, 300, 500, \cdots, 1900\}$ & 1900 & 10 \\
 		Criterion & Entropy, Gini Coef. & Gini Coef. & 2 \\
 		Max Features ($\phi$) & $\log_2(p), \sqrt{p}$ & $\log_2(p)$ & 2\\
 		\hline 
 		All Combinations& & & 280
		\end{tabular}
		\caption{Hyper-parameter Scope and Result for Random Forest}
 	\end{table}
 	As mentioned before, all 280 candidates were evaluated using 5-CV and cross entropy loss. The best combination of hyper-parameters found after 1400 model fittings was included in the table. Note that the optimal value of $\tau$ was found on the boundary of $\mc{G}$, it is reasonable to suspect that further increase of $\tau$ could lead to an even further improvement in model performance. To accommodate this issue, two additional models with $\tau=2000, 2100$ were evaluated using 5-CV and compared with the best model identified using grid search. It turned out that the improvements in generalization error were both only around $0.4\%$, and the performance even dropped when $\tau$ was risen from $2000$ to $2100$. Therefore, the model identified from grid searching was chosen to represent random forest class.
 	
 	\begin{table}[H]
	 	\centering
		\begin{tabular}{l|c}
			$\tau$ & Avg. Test Err. ($\pm \frac{1}{2}$ Range) \\
			\hline
			1900 & $0.3709(\pm 0.008014)$ \\
			2000 & $0.3693(\pm 0.005638)$ \\
			2100 & $0.3694(\pm 0.01037)$ 
		\end{tabular}
		\caption{Further Increments of Number of Estimators}
	\end{table}
 
 	\subsection{Ensemble Model: Gradient Boosting}
 	\paragraph{} Gradient boosting is another type of ensemble models, in contrast to the parallel ensemble strategy of random forests, GBs ensemble multiple models vertically. For GBs, Friedman mean squared error was used to measure the quality of splits in the boosting process. The table below presents 
 	\begin{table}
 		\centering
 		\begin{tabular}{l|c|c|c}
 		H-Param & Range & Best & Total \\
 		\hline
 			
 		\end{tabular}
 		\caption{Hyper-parameter Scope and Result for Gradient Boosting Machines}
 	\end{table}
 	
 	\subsection{Support Vector Classifier}
 	
 	\subsection{Deep Neural Network}
 	
 	\subsection{Calibration}
 	
 	\subsection{Threshold and Accuracy}
 	
	\section{Appendix}
\end{document}










